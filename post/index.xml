<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts | MLO Lab</title><link>https://mlo-lab.github.io/post/</link><atom:link href="https://mlo-lab.github.io/post/index.xml" rel="self" type="application/rss+xml"/><description>Posts</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2025 MLO Lab</copyright><lastBuildDate>Thu, 04 Dec 2025 13:58:55 +0100</lastBuildDate><image><url>https://mlo-lab.github.io/media/logo_huedbb8239ffd36c55e33765ff7c80fb90_78702_300x300_fit_lanczos_2.png</url><title>Posts</title><link>https://mlo-lab.github.io/post/</link></image><item><title>Insights into FLAME: Visit by Dr. Marius Herr</title><link>https://mlo-lab.github.io/post/flame-project-herr/</link><pubDate>Thu, 04 Dec 2025 13:58:55 +0100</pubDate><guid>https://mlo-lab.github.io/post/flame-project-herr/</guid><description>&lt;p>We hosted Dr. Marius Herr from the University of Tübingen, who presented FLAME — &lt;a href="https://docs.privateaim.net" target="_blank" rel="noopener">Federated Learning and Analyses in Medicine&lt;/a>, a platform developed within the privateAIM initiative. He outlined how patient data can be analyzed in a privacy-preserving manner and demonstrated how our methods could be applied to clinical data through FLAME.
The subsequent discussions provided practical insights into the requirements for clinically ready applications of such systems.&lt;/p>
&lt;p>We warmly thank Dr. Herr for his visit and the constructive exchange. We look forward to staying in contact and exploring potential collaborations as FLAME moves toward national implementation.&lt;/p></description></item><item><title>Welcome to our newest team member, Azza</title><link>https://mlo-lab.github.io/post/azza-welcome-to-the-team/</link><pubDate>Mon, 17 Nov 2025 10:38:11 +0100</pubDate><guid>https://mlo-lab.github.io/post/azza-welcome-to-the-team/</guid><description>&lt;p>Azza joins our ML sub-team as a doctoral researcher working on uncertainty quantification and estimation for trustworthy AI. She brings experience in applied machine learning and language model engineering and will support our ongoing research on reliable ML methods in oncology.&lt;/p></description></item><item><title>Our paper “Fine-Grained Uncertainty Decomposition in Large Language Models: A Spectral Approach” is now available at AAAI</title><link>https://mlo-lab.github.io/post/walha-uncertainty-decomposition/</link><pubDate>Sun, 16 Nov 2025 14:18:17 +0100</pubDate><guid>https://mlo-lab.github.io/post/walha-uncertainty-decomposition/</guid><description>&lt;p>This work presents Spectral Uncertainty, a new way to decompose uncertainty in large language models. Using the Von Neumann entropy, the method distinguishes aleatoric from epistemic uncertainty and incorporates detailed semantic structure in model outputs. Across multiple benchmarks, it outperforms current approaches in estimating uncertainty.&lt;/p></description></item><item><title>Florian named among the world’s most cited researchers — huge congrats to the whole MLO Lab team!</title><link>https://mlo-lab.github.io/post/buettner-most-cited-2025/</link><pubDate>Fri, 14 Nov 2025 14:41:04 +0100</pubDate><guid>https://mlo-lab.github.io/post/buettner-most-cited-2025/</guid><description>&lt;p>A proud moment: Florian has been listed as a Highly Cited Researcher 2025, placing him among the top 1% of scientists worldwide. This recognition reflects the shared work, ideas, and energy that move our lab forward every day. Proud to have this team! &lt;a href="https://aktuelles.uni-frankfurt.de/en/english/seven-goethe-university-researchers-among-the-most-cited-scientists-in-the-world/" target="_blank" rel="noopener">Official Announcement&lt;/a>&lt;/p></description></item><item><title>Welcome to the team, Hendrik</title><link>https://mlo-lab.github.io/post/hendrik-welcome-to-the-team/</link><pubDate>Sat, 18 Oct 2025 09:29:04 +0100</pubDate><guid>https://mlo-lab.github.io/post/hendrik-welcome-to-the-team/</guid><description>&lt;p>Hendrik joins our ML sub-team, working on machine learning methods that combine large-scale EHR data with other data types such as genomics. He focuses on causality and uncertainty estimation and will support our ongoing methodological work.&lt;/p></description></item><item><title>Welcome to the team, Yussuf</title><link>https://mlo-lab.github.io/post/yussuf-welcome-to-the-team/</link><pubDate>Wed, 01 Oct 2025 09:36:24 +0100</pubDate><guid>https://mlo-lab.github.io/post/yussuf-welcome-to-the-team/</guid><description>&lt;p>Yussuf joins our bioinformatics sub-team as a doctoral researcher working on interpretable probabilistic machine learning models for multimodal data integration. He applies these methods to projects on novel mRNA technologies for colorectal and pancreatic cancer and will support our ongoing research efforts.&lt;/p></description></item><item><title>Our paper “Learning interpretable representations of single-cell multi-omics data with multi-output Gaussian processes” has been published in Nucleic Acids Research.</title><link>https://mlo-lab.github.io/post/moslehi-multioutput-gp/</link><pubDate>Tue, 12 Aug 2025 16:11:09 +0100</pubDate><guid>https://mlo-lab.github.io/post/moslehi-multioutput-gp/</guid><description>&lt;p>We present a unified framework that combines expressive neural embeddings with interpretable multi-output Gaussian processes for single-cell genomics. Joint representations of cells and genes reveal meaningful links between cell clusters and their marker genes via an interpretable gene-relevance map. &lt;a href="https://academic.oup.com/nar/article/53/14/gkaf630/8210588" target="_blank" rel="noopener">Published in Nucleic Acids Research&lt;/a>.&lt;/p></description></item><item><title>An autonomous agent for auditing and improving the reliability of clinical AI models — now published.</title><link>https://mlo-lab.github.io/post/kuhn-autonomous-agent-clinical-ai/</link><pubDate>Tue, 08 Jul 2025 14:41:12 +0100</pubDate><guid>https://mlo-lab.github.io/post/kuhn-autonomous-agent-clinical-ai/</guid><description>&lt;p>We introduce ModelAuditor, a self-reflective agent that simulates clinically relevant distribution shifts and produces interpretable reports on likely failure modes. Across multiple medical imaging domains, it recovers up to 25% of performance lost under shift while providing actionable deployment insights.&lt;/p></description></item><item><title>Application-driven validation of posteriors in inverse problems, published in Medical Image Analysis.</title><link>https://mlo-lab.github.io/post/buettner-posterior-inverse-problems/</link><pubDate>Tue, 01 Apr 2025 15:10:20 +0100</pubDate><guid>https://mlo-lab.github.io/post/buettner-posterior-inverse-problems/</guid><description>&lt;p>We present the first systematic framework for application-driven validation of posterior-based methods in inverse problems. Adapting concepts from object detection enables mode-centric validation with interpretable, application-focused metrics, demonstrated on multiple medical imaging use cases. Published in Medical Image Analysis.&lt;/p></description></item><item><title>Our paper on bidirectional human-AI visual alignment is out at the ICLR 2025 Workshop!</title><link>https://mlo-lab.github.io/post/buettner-lvlm-aided-visiual-alignment/</link><pubDate>Thu, 06 Mar 2025 15:34:56 +0100</pubDate><guid>https://mlo-lab.github.io/post/buettner-lvlm-aided-visiual-alignment/</guid><description>&lt;p>We introduce LVLM-Aided Visual Alignment (LVLM-VA), which aligns small vision models with human domain knowledge using large vision-language models. A bidirectional interface translates model behavior into natural language and expert instructions into image-level critiques, improving performance while reducing fine-grained feedback needs. Published at the ICLR 2025 Workshop on Bidirectional Human-AI Alignment.&lt;/p></description></item><item><title>Forget forgetting — our TMLR paper shows how uncertainty helps models keep their memory straight!</title><link>https://mlo-lab.github.io/post/serra-predictive-uncertainty-catastrophic-forgetting/</link><pubDate>Tue, 04 Mar 2025 15:32:51 +0100</pubDate><guid>https://mlo-lab.github.io/post/serra-predictive-uncertainty-catastrophic-forgetting/</guid><description>&lt;p>We analyze how predictive uncertainty can guide memory management to mitigate catastrophic forgetting and introduce a generalized-variance–based uncertainty measure. Uncertainty-aware sampling improves retention across tasks. Published in the Journal of Transactions on Machine Learning Research.&lt;/p></description></item><item><title>New at AISTATS: IUPM — a label-free method for reliable model monitoring under drift.</title><link>https://mlo-lab.github.io/post/buettner-iupm-intervention/</link><pubDate>Wed, 22 Jan 2025 15:47:44 +0100</pubDate><guid>https://mlo-lab.github.io/post/buettner-iupm-intervention/</guid><description>&lt;p>We propose IUPM, a label-free method for tracking performance under gradual distribution shifts using optimal transport. IUPM quantifies uncertainty in its estimates and guides targeted labeling to restore reliability, outperforming existing baselines across scenarios. Published in the Proceedings of the 28th International Conference on Artificial Intelligence and Statistics (AISTATS).&lt;/p></description></item><item><title>Our ICLR paper proves that not everything needs to be forgotten — tackling catastrophic forgetting head-on!</title><link>https://mlo-lab.github.io/post/serra-continous-learning-memory-managment/</link><pubDate>Wed, 22 Jan 2025 15:24:57 +0100</pubDate><guid>https://mlo-lab.github.io/post/serra-continous-learning-memory-managment/</guid><description>&lt;p>We propose an uncertainty-aware memory-based approach for online Federated Continual Learning. Using a Bregman Information estimator to guide selective replay, the method reduces catastrophic forgetting across modalities while preserving privacy and communication efficiency. Presented at the Thirteenth International Conference on Learning Representations (ICLR).&lt;/p></description></item><item><title>Unsupervised and efficient — our latest work exposes and mitigates shortcut learning!</title><link>https://mlo-lab.github.io/post/kuhn-unsupervised-shortcut-transformers/</link><pubDate>Wed, 01 Jan 2025 15:16:16 +0100</pubDate><guid>https://mlo-lab.github.io/post/kuhn-unsupervised-shortcut-transformers/</guid><description>&lt;p>We introduce an unsupervised framework to detect and mitigate shortcut learning in transformers. The method improves both worst-group and average accuracy while reducing annotation effort, offering interpretable insights for experts and running efficiently on consumer hardware.&lt;/p></description></item><item><title>Our latest work on deep learning for metabolomics just appeared in Scientific Reports!</title><link>https://mlo-lab.github.io/post/buettner-metabolic-changes-graph-embeddings/</link><pubDate>Thu, 28 Nov 2024 13:26:13 +0100</pubDate><guid>https://mlo-lab.github.io/post/buettner-metabolic-changes-graph-embeddings/</guid><description>&lt;p>We introduce GEMNA, a deep learning framework for mass spectrometry–based metabolomics that uses graph and edge embeddings with anomaly detection. GEMNA outperforms traditional tools in untargeted studies, producing clearer clusters and improved biological insights. Published in &lt;a href="https://www.nature.com/articles/s41598-024-80955-5" target="_blank" rel="noopener">Scientific Reports&lt;/a>&lt;/p></description></item><item><title>Rounding out our research hat trick with new insights into interpretable image synthesis!</title><link>https://mlo-lab.github.io/post/gruber-diagnostic_image_generators/</link><pubDate>Mon, 02 Sep 2024 11:27:52 +0100</pubDate><guid>https://mlo-lab.github.io/post/gruber-diagnostic_image_generators/</guid><description>&lt;p>Our latest work presents a new approach to disentangle image generation performance by decomposing cosine similarity into cluster-level contributions using central kernel alignment. This allows us to quantify how different pixel regions contribute to overall image quality, enabling more fine-grained evaluation and improved explainability of generative models across real-world use cases. Published at the Interpretable AI: Past, Present and Future Workshop at NeurIPS 2024.&lt;/p></description></item><item><title>Our framework for explanatory model monitoring was featured at KDD</title><link>https://mlo-lab.github.io/post/buettner-feature-shift-performance/</link><pubDate>Sat, 24 Aug 2024 14:57:12 +0100</pubDate><guid>https://mlo-lab.github.io/post/buettner-feature-shift-performance/</guid><description>&lt;p>We introduce Explanatory Performance Estimation (XPE), a method that explains model behavior under feature shifts by linking performance changes to interpretable input characteristics using Optimal Transport and Shapley Values. This enables explanatory model monitoring across image, audio, and tabular data. Published in the Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.&lt;/p></description></item><item><title>Our latest paper at HCI explores how human gaze can make AI more interpretable and reliable!</title><link>https://mlo-lab.github.io/post/buettner-expert-eyes-algning-industrial-ai/</link><pubDate>Mon, 29 Jul 2024 14:21:54 +0100</pubDate><guid>https://mlo-lab.github.io/post/buettner-expert-eyes-algning-industrial-ai/</guid><description>&lt;p>We show how integrating human gaze information aligns human and machine attention, improving model robustness and explainability. Demonstrated on real-world visual quality inspection, the approach highlights the value of explicit human knowledge in training trustworthy AI. Published at the &lt;a href="https://link.springer.com/book/10.1007/978-3-031-60611-3" target="_blank" rel="noopener">International Conference on Human-Computer Interaction&lt;/a>.&lt;/p></description></item><item><title>New at ICML: improving the stability of feature attributions through optimal combinations!</title><link>https://mlo-lab.github.io/post/buettner-provably-better-explanations/</link><pubDate>Sun, 07 Jul 2024 14:14:20 +0100</pubDate><guid>https://mlo-lab.github.io/post/buettner-provably-better-explanations/</guid><description>&lt;p>We improve the quality of feature attributions by optimally combining multiple explanation methods. Our convex-combination strategy enhances robustness and faithfulness, consistently outperforming individual methods and baselines across architectures. Published at the International Conference on Machine Learning.&lt;/p></description></item><item><title>Advancing global understanding of evaluation metrics — now published in Nature Methods!</title><link>https://mlo-lab.github.io/post/buettner-metric-related-pitfalls/</link><pubDate>Mon, 12 Feb 2024 14:24:23 +0100</pubDate><guid>https://mlo-lab.github.io/post/buettner-metric-related-pitfalls/</guid><description>&lt;p>This work compiles a domain-agnostic taxonomy of pitfalls in validation metrics, based on a multistage Delphi process and community feedback. It offers practical guidance to improve evaluation practices across application domains. Published in &lt;a href="https://www.nature.com/articles/s41592-023-02150-0" target="_blank" rel="noopener">Nature Methods&lt;/a>.&lt;/p></description></item><item><title>Metrics Reloaded — now published in Nature Methods.</title><link>https://mlo-lab.github.io/post/buettner-metrics-reloaded-image-validation/</link><pubDate>Mon, 12 Feb 2024 14:10:16 +0100</pubDate><guid>https://mlo-lab.github.io/post/buettner-metrics-reloaded-image-validation/</guid><description>&lt;p>We present Metrics Reloaded, a comprehensive framework for problem-aware selection of evaluation metrics in biomedical image analysis. Developed through a large international Delphi process, it introduces the concept of a problem fingerprint to guide researchers toward meaningful and domain-relevant validation. Published in &lt;a href="https://www.nature.com/articles/s41592-023-02151-z" target="_blank" rel="noopener">Nature Methods&lt;/a>.&lt;/p></description></item><item><title>Our kernel uncertainty framework is in at ICML!</title><link>https://mlo-lab.github.io/post/gruber-kernel-scores/</link><pubDate>Mon, 09 Oct 2023 10:34:16 +0100</pubDate><guid>https://mlo-lab.github.io/post/gruber-kernel-scores/</guid><description>&lt;p>Our latest work introduces the first bias–variance–covariance decomposition for kernel scores, providing a unified framework for uncertainty estimation in generative models. We show how kernel-based entropy and variance capture uncertainty across image, audio, and language generation — even in closed-source models. Published at the International Conference on Machine Learning.&lt;/p></description></item><item><title>Paper accepted at ICML CompBio 2023!</title><link>https://mlo-lab.github.io/post/qoku-cellij-icml-23/</link><pubDate>Tue, 25 Jul 2023 22:16:23 +0200</pubDate><guid>https://mlo-lab.github.io/post/qoku-cellij-icml-23/</guid><description>&lt;p>&lt;strong>Paper accepted! Our latest work on developing a versatile framework for rapid prototyping and training of a wide range of factor analysis models for multi-omics data got accepted at this year&amp;rsquo;s ICML workshop on computational biology!&lt;/strong>&lt;/p></description></item><item><title>ERC Consolidator Grant for Florian!</title><link>https://mlo-lab.github.io/post/buettner-erc-grant-23/</link><pubDate>Sun, 22 Jan 2023 16:16:26 +0200</pubDate><guid>https://mlo-lab.github.io/post/buettner-erc-grant-23/</guid><description>&lt;p>&lt;strong>With 2 million Euros in funding from the European Research Council, we will be developing AI models to support doctors in the diagnosis and treatment of cancer. &lt;a href="https://www.dkfz.de/de/presse/pressemitteilungen/2023/dkfz-pm-23-07-ERC-Consolidator-Grant-fuer-DKTK-Forscher-Florian-Buettner.php" target="_blank" rel="noopener">Read more&lt;/a>!&lt;/strong>&lt;/p></description></item><item><title>Paper accepted at AISTATS 2023!</title><link>https://mlo-lab.github.io/post/gruber-uncertainty-aistats-23/</link><pubDate>Fri, 20 Jan 2023 16:16:03 +0200</pubDate><guid>https://mlo-lab.github.io/post/gruber-uncertainty-aistats-23/</guid><description>&lt;p>&lt;strong>Paper accepted! Our latest work on a general bias-variance decomposition for proper scores got accepted at this year&amp;rsquo;s AISTATS conference!&lt;/strong>&lt;/p></description></item><item><title>Paper accepted at AISTATS 2023!</title><link>https://mlo-lab.github.io/post/qoku-encoding-aistats-23/</link><pubDate>Fri, 20 Jan 2023 16:15:47 +0200</pubDate><guid>https://mlo-lab.github.io/post/qoku-encoding-aistats-23/</guid><description>&lt;p>&lt;strong>Paper accepted! Our latest work on multi-view latent variable models with structured sparsity got accepted at this year&amp;rsquo;s AISTATS conference!&lt;/strong>&lt;/p></description></item><item><title>Paper accepted at AAAI 2023!</title><link>https://mlo-lab.github.io/post/hekler-test-aaai-23/</link><pubDate>Sat, 14 Jan 2023 16:15:05 +0200</pubDate><guid>https://mlo-lab.github.io/post/hekler-test-aaai-23/</guid><description>&lt;p>&lt;strong>Paper accepted! Our latest work on quantifying uncertainty under real-world conditions got accepted at this year&amp;rsquo;s AAAI conference on artificial intelligence!&lt;/strong>&lt;/p></description></item><item><title>Paper accepted at ECCV 2022!</title><link>https://mlo-lab.github.io/post/tomani-eccv-22/</link><pubDate>Fri, 08 Jul 2022 17:13:49 +0200</pubDate><guid>https://mlo-lab.github.io/post/tomani-eccv-22/</guid><description>&lt;p>&lt;strong>Paper accepted! Our latest work on boosting the expressive power in post-hoc uncertainty calibration got accepted at this year&amp;rsquo;s ECCV!&lt;/strong>&lt;/p></description></item><item><title>Paper accepted at Cancer Cell 2022!</title><link>https://mlo-lab.github.io/post/wolf-aml-cancercell-22/</link><pubDate>Fri, 08 Jul 2022 16:33:37 +0200</pubDate><guid>https://mlo-lab.github.io/post/wolf-aml-cancercell-22/</guid><description>&lt;p>&lt;strong>Paper accepted! Our latest work on characterizing proteogenomic subtypes of AML got accepted at this year&amp;rsquo;s Cancer Cell!&lt;/strong>&lt;/p></description></item><item><title>Paper accepted at KDD 2021!</title><link>https://mlo-lab.github.io/post/spexlvm-kdd/</link><pubDate>Tue, 13 Jul 2021 16:30:04 +0200</pubDate><guid>https://mlo-lab.github.io/post/spexlvm-kdd/</guid><description>&lt;p>Latent variable models are powerful statistical tools that can uncover relevant variation between patients or cells, by inferring unobserved hidden states from observable high-dimensional data. A major shortcoming of current methods, however, is their inability to learn sparse and interpretable hidden states. Additionally, in settings where partial knowledge on the latent structure of the data is readily available, a statistically sound integration of prior information into current methods is challenging. To address these issues, we propose spex-LVM, a factorial latent variable model with &lt;strong>s&lt;/strong>parse &lt;strong>p&lt;/strong>riors to encourage the inference of &lt;strong>ex&lt;/strong>plainable factors driven by domain-relevant information. spex-LVM utilizes existing knowledge of curated biomedical pathways to automatically assign annotated attributes to latent factors, yielding interpretable results tailored to the corresponding domain of interest. Evaluations on simulated and real single-cell RNA-seq datasets demonstrate that our model robustly identifies relevant structure in an inherently explainable manner, distinguishes technical noise from sources of biomedical variation, and provides data-driven adaptations of existing pathway annotations.&lt;/p></description></item><item><title>Paper accepted at UAI!</title><link>https://mlo-lab.github.io/post/paper-uai-yang/</link><pubDate>Thu, 20 May 2021 16:59:59 +0200</pubDate><guid>https://mlo-lab.github.io/post/paper-uai-yang/</guid><description>&lt;p>&lt;strong>Paper accepted! Our latest work on multi-output Gaussian Process Latent Variable models got accepted at this year&amp;rsquo;s UAI!&lt;/strong>&lt;/p></description></item><item><title>Workshop on XAI and Trustworthiness in Healthcare at KDD 2021!</title><link>https://mlo-lab.github.io/post/workshop/</link><pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate><guid>https://mlo-lab.github.io/post/workshop/</guid><description>&lt;p>&lt;strong>We organize a workshop at this year&amp;rsquo;s KDD&lt;/strong>&lt;/p>
&lt;p>Check out the workshop website &lt;a href="https://dshealthkdd.github.io/dshealth-2021/" target="_blank">here&lt;/a>.
Paper submission deadline is 10th May 2021.&lt;/p>
&lt;h1 id="overview">Overview&lt;/h1>
&lt;p>Healthcare is, traditionally, a knowledge-driven enterprise with an enormous amount of data - both structured and unstructured. These data can impact positively on the development of data-driven health care including precision medicine and precision public health. In recent years, large scale medical/clinical datasets, such as “omics” data and radiology reports are increasingly available. We have also witnessed an increasing number of successful AI/ML applications using such datasets to address problems such as drug repurposing and preliminary screening of radiology reports. To facilitate the adoption of such AI/ML in practice, we have simultaneously witnessed an increasing adoption/innovation of using explainability methods to analyze/present AI for Health. In this deep learning era, What is the current status of AI/ML applications in healthcare? What are the standard methods of explaining such AI models for healthcare? What are the roles of causality in AI/ML practices? What are the state-of-the-art developments in causal AI in health and health care domains? What are the limitations and how are the different facets of trust and explanations (see figure 1 below) being addressed in practice? Can knowledge-backed AI lead to more robust and interpretable models? How do data scientists and physicians apply this knowledge in collaboration and via human-centered AI approaches to further the field and improve healthcare? How are regulatory requirements for transparency and trustworthiness of models and data privacy being defined and how can they be fulfilled? After witnessing so many great achievements from deep learning lately, we propose to invite world-leading experts from both data science and healthcare to discuss and debate the path forward for practical applications of AI/ML in healthcare, including demos, early work, and critiques on the current state and the path forward for explainability and trustworthiness in AI. More specifically, we plan to attract high-quality original research from emerging areas with significant implications in healthcare and invite open discussions on controversial yet crucial topics regarding healthcare transformation&lt;/p>
&lt;h1 id="key-dates">Key dates&lt;/h1>
&lt;ul>
&lt;li>Paper Submission opens: Apr 15, 2021&lt;/li>
&lt;li>Paper Submission deadline: May 10, 2021&lt;/li>
&lt;li>Acceptance Notice: Jun 10, 2021&lt;/li>
&lt;li>Workshop Date: Aug 14, 2021&lt;/li>
&lt;/ul>
&lt;p>All deadlines correspond to 11:59 PM Hawaii Standard Time ( HST).&lt;/p></description></item></channel></rss>