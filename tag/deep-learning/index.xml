<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>deep learning | MLO Lab</title><link>https://mlo-lab.github.io/tag/deep-learning/</link><atom:link href="https://mlo-lab.github.io/tag/deep-learning/index.xml" rel="self" type="application/rss+xml"/><description>deep learning</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2025 MLO Lab</copyright><lastBuildDate>Fri, 09 Apr 2021 00:00:00 +0000</lastBuildDate><image><url>https://mlo-lab.github.io/media/logo_huedbb8239ffd36c55e33765ff7c80fb90_78702_300x300_fit_lanczos_2.png</url><title>deep learning</title><link>https://mlo-lab.github.io/tag/deep-learning/</link></image><item><title>Probabilistic latent variable models</title><link>https://mlo-lab.github.io/project/lvmodels/</link><pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate><guid>https://mlo-lab.github.io/project/lvmodels/</guid><description>&lt;p>Latent variable models (LVMs) are a statistical tool to infer an unobserved, hidden state of a complex (e.g. biological) system based on observable data that is often high-dimensional. To this end, a high-dimensional dataset of correlated observations is reduced into a low-dimensional dataset of uncorrelated and interpretable latent variables. Probabilistic approaches allow for a principled way to disentangle distinct sources of variation and explicitly model dependencies between features as well as samples.&lt;/p>
&lt;h2 id="accounting-for-dependencies-between-genes-in-lvms">Accounting for dependencies between genes in LVMs&lt;/h2>
&lt;ul>
&lt;li>Standard latent variable models only model dependencies between samples&lt;/li>
&lt;li>Can we make dependencies between features (genes) explicit?&lt;/li>
&lt;li>Use framework of Gaussian Process Latent Variable Models (GP-LVM)&lt;/li>
&lt;li>Probabilistic kernel PCA via GP regression with unobserved input&lt;/li>
&lt;li>Introduce kernel to model covariance between genes&lt;/li>
&lt;li>Learn latent variables for genes and samples and connect via Kronecker Product&lt;/li>
&lt;li>Apply to matrix completion tasks&lt;/li>
&lt;/ul>
&lt;p>Reference: Yang &amp;amp; Buettner, UAI 2021 (in revision)&lt;/p>
&lt;h2 id="hierarchical-autoencoders-for-domain-generalisation">Hierarchical autoencoders for Domain Generalisation&lt;/h2>
&lt;ul>
&lt;li>Learn VAE to disentangle domain-specific information form class-specific information and residual variance&lt;/li>
&lt;li>Place Dirichlet prior on domain representation&lt;/li>
&lt;li>Learn “topics” that describe domain structure in unsupervised manner&lt;/li>
&lt;li>Interpretable model for unsupervised domain generalisation&lt;/li>
&lt;/ul>
&lt;p>Reference: Sun &amp;amp; Buettner, ICLR Workshop on robustML, 2021&lt;/p></description></item><item><title>Uncertainty-aware deep learning in the real world</title><link>https://mlo-lab.github.io/project/calibration/</link><pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate><guid>https://mlo-lab.github.io/project/calibration/</guid><description>&lt;p>Due to their high predictive power, deep neural networks are increasingly being used as part of decision
making systems in real world applications. However, such systems require not only high accuracy, but also reliable and calibrated uncertainty estimates. Especially in safety critical applications in medicine where average case performance is insufficient, practitioners need to have access to reliable predictive uncertainty during the entire lifecycle of the model. This means confidence scores (or
predictive uncertainty) should be meaningful not only for in-domain predictions, but also under gradual
domain drift where the distribution of the input samples gradually changes from in-domain to truly out-ofdistribution (OOD). In healthcare, common examples of such domain shift scenarios are a patient demographic that changes over time or new hospitals in which a model is to be deployed.&lt;/p>
&lt;h2 id="towards-trustworthy-predictions-from-deep-neural-networks-with-fast-adversarial-calibration">Towards Trustworthy Predictions from Deep Neural Networks with Fast Adversarial Calibration&lt;/h2>
&lt;p>Here, we propose an efficient yet general modelling approach for obtaining wellcalibrated, trustworthy probabilities for samples obtained after a domain shift. We introduce a new training strategy combining an entropy-encouraging loss term with an adversarial calibration loss term and demonstrate that this results in well-calibrated and technically trustworthy predictions for a wide range of domain drifts. We comprehensively evaluate previously proposed approaches on different data modalities, a large range of data sets including sequence data, network architectures and perturbation strategies. We observe
that our modelling approach substantially outperforms existing state-of-the-art approaches, yielding well-calibrated predictions under domain drift.&lt;/p>
&lt;p>&lt;em>Reference: Tomani &amp;amp; Buettner, AAAI 2021&lt;/em>&lt;/p>
&lt;h2 id="post-hoc-uncertainty-calibration-for-domain-drift-scenarios">Post-hoc Uncertainty Calibration for Domain Drift Scenarios&lt;/h2>
&lt;p>We address the problem of uncertainty calibration. While standard deep neural networks typically yield uncalibrated predictions, calibrated confidence scores that are representative of the true likelihood of a prediction can be achieved using post-hoc calibration methods. However, to date the focus of these approaches has been on in-domain calibration. Our contribution is two-fold.
First, we show that existing post-hoc calibration methods yield highly over-confident predictions under domain shift. Second, we introduce a simple strategy where perturbations are applied to samples in the validation set before performing the post-hoc calibration step. In extensive experiments, we demonstrate that this perturbation step results in substantially better calibration under domain shift on a wide range of architectures and modelling tasks.&lt;/p>
&lt;p>&lt;em>Reference: Tomani, Gruber, Erdem, Cremers &amp;amp; Buettner, CVPR 2021 (oral presentation)&lt;/em>&lt;/p>
&lt;h2 id="parameterized-temperature-scaling-for-boosting-the-expressive-power-in-post-hoc-uncertainty-calibration">Parameterized Temperature Scaling for Boosting the Expressive Power in Post-Hoc Uncertainty Calibration&lt;/h2>
&lt;p>We address the problem of uncertainty calibration and introduce a novel calibration
method, Parametrized Temperature Scaling (PTS). Standard deep neural networks typically yield uncalibrated predictions, which can be transformed into calibrated confidence
scores using post-hoc calibration methods. In this contribution, we demonstrate that the
performance of accuracy-preserving state-ofthe-art post-hoc calibrators is limited by their
intrinsic expressive power. We generalize temperature scaling by computing predictionspecific temperatures, parameterized by a neural network. We show with extensive experiments that our novel accuracy-preserving approach consistently outperforms existing algorithms across a large number of model
architectures, datasets and metrics.&lt;/p>
&lt;p>&lt;em>Reference: Tomani, Cremers &amp;amp; Buettner, arxiv 2021 (in submission)&lt;/em>&lt;/p></description></item></channel></rss>